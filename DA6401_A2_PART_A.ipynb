{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        image_size: Tuple[int, int] = (224, 224),\n",
    "        batch_size: int = 64,\n",
    "        val_split: float = 0.2,\n",
    "        use_augmentation: bool = False,\n",
    "        num_workers: int = 4,\n",
    "        seed: int = 42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom Data Module for handling dataset loading, transformation, and splitting.\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str): Path to dataset directory.\n",
    "            image_size (Tuple[int, int]): Target image size (height, width).\n",
    "            batch_size (int): Batch size for DataLoader.\n",
    "            val_split (float): Fraction of training data to use for validation.\n",
    "            use_augmentation (bool): Whether to apply data augmentation.\n",
    "            num_workers (int): Number of workers for DataLoader.\n",
    "            seed (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.class_names = []\n",
    "        \n",
    "        # Define transforms\n",
    "        self.train_transform = self._get_train_transform()\n",
    "        self.test_transform = self._get_test_transform()\n",
    "\n",
    "        # Set manual seeds for reproducibility\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "    def _get_train_transform(self):\n",
    "        \"\"\"Defines transformation pipeline for training data.\"\"\"\n",
    "        if self.use_augmentation:\n",
    "            return transforms.Compose([\n",
    "                transforms.RandomResizedCrop(self.image_size[0]),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), shear=10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            return transforms.Compose([\n",
    "                transforms.Resize(self.image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def _get_test_transform(self):\n",
    "        \"\"\"Defines transformation pipeline for validation and test data.\"\"\"\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def _split_train_val(self, dataset):\n",
    "        \"\"\"Splits dataset into training and validation sets while preserving class distribution.\"\"\"\n",
    "        total_size = len(dataset)\n",
    "        indices = torch.randperm(total_size).tolist()\n",
    "\n",
    "        val_size = int(total_size * self.val_split)\n",
    "        train_indices, val_indices = indices[val_size:], indices[:val_size]\n",
    "\n",
    "        return train_indices, val_indices\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"Loads datasets and applies transformations.\"\"\"\n",
    "        full_dataset = datasets.ImageFolder(root=self.data_dir, transform=self.train_transform)\n",
    "        self.class_names = full_dataset.classes\n",
    "\n",
    "        train_idx, val_idx = self._split_train_val(full_dataset)\n",
    "\n",
    "        # Create subsets\n",
    "        self.train_dataset = Subset(full_dataset, train_idx)\n",
    "        self.val_dataset = Subset(datasets.ImageFolder(root=self.data_dir, transform=self.test_transform), val_idx)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Returns DataLoader for training data.\"\"\"\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Returns DataLoader for validation data.\"\"\"\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self, test_dir: Optional[str] = None):\n",
    "        \"\"\"Returns DataLoader for test data.\"\"\"\n",
    "        test_path = Path(test_dir) if test_dir else self.data_dir.parent / \"val\"\n",
    "        test_dataset = datasets.ImageFolder(root=test_path, transform=self.test_transform)\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches: 125\n",
      "Validation batches: 32\n",
      "Test batches: 32\n",
      "Class Names: ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "# if __name__ == \"__main__\":\n",
    "data_module = CustomDataModule(data_dir=\"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_2/inaturalist_12K/train\",\n",
    "                                use_augmentation=True,\n",
    "                                val_split= 0.2,\n",
    "                                batch_size= 64\n",
    "                                )\n",
    "data_module.setup()\n",
    "\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "print(f\"Class Names: {data_module.class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class CustomCNN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int, int, int] = (3, 224, 224),\n",
    "        num_classes: int = 10,\n",
    "        first_layer_filters: int = 32,\n",
    "        filter_org: float = 2.0,\n",
    "        kernel_size: int = 3,\n",
    "        conv_layers: int = 4,\n",
    "        activation: str = \"relu\",\n",
    "        dropout: float = 0.3,\n",
    "        batch_norm: bool = True,\n",
    "        dense_size: int = 128,\n",
    "        learning_rate: float = 1e-3,\n",
    "        optimizer_name: str = \"adam\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom CNN Model\n",
    "\n",
    "        Args:\n",
    "            input_shape (Tuple[int, int, int]): (Channels, Height, Width)\n",
    "            num_classes (int): Number of output classes\n",
    "            first_layer_filters (int): Number of filters in the first layer\n",
    "            filter_org (float): Scaling factor for filters per layer\n",
    "            kernel_size (int): Size of the convolution kernel\n",
    "            conv_layers (int): Number of convolutional layers\n",
    "            activation (str): Activation function to use (relu, gelu, mish, silu)\n",
    "            dropout (float): Dropout rate\n",
    "            batch_norm (bool): Whether to use batch normalization\n",
    "            dense_size (int): Number of neurons in the fully connected layer\n",
    "            learning_rate (float): Learning rate\n",
    "            optimizer_name (str): Optimizer type (adam, sgd, etc.)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Choose activation function\n",
    "        activations = {\n",
    "            \"relu\": nn.ReLU(),\n",
    "            \"gelu\": nn.GELU(),\n",
    "            \"silu\": nn.SiLU(),\n",
    "            \"mish\": nn.Mish()\n",
    "        }\n",
    "        self.activation = activations.get(activation, nn.ReLU())\n",
    "\n",
    "        # CNN Layers\n",
    "        layers = []\n",
    "        in_channels = input_shape[0]\n",
    "        filters = first_layer_filters\n",
    "\n",
    "        for _ in range(conv_layers):\n",
    "            layers.append(nn.Conv2d(in_channels, filters, kernel_size, padding=1))\n",
    "            if batch_norm:\n",
    "                layers.append(nn.BatchNorm2d(filters))\n",
    "            layers.append(self.activation)\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            in_channels = filters\n",
    "            filters = int(filters * filter_org)  # Ensure filters remain an integer\n",
    "\n",
    "        self.conv_block = nn.Sequential(*layers)\n",
    "\n",
    "        # Calculate output feature map size\n",
    "        feature_map_size = input_shape[1] // (2 ** conv_layers)  # Assuming max pooling halves the size each time\n",
    "        if feature_map_size <= 0:\n",
    "            raise ValueError(\"Too many pooling layers, feature map size is zero!\")\n",
    "\n",
    "        final_filters = in_channels  # Last number of filters used in conv layers\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(in_features=int(final_filters * feature_map_size * feature_map_size), out_features=dense_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(dense_size, num_classes)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass.\"\"\"\n",
    "        x = self.conv_block(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step for Lightning.\"\"\"\n",
    "        images, labels = batch\n",
    "\n",
    "        # Convert one-hot to class index if needed\n",
    "        labels = labels.argmax(dim=1) if labels.ndim == 2 else labels  \n",
    "\n",
    "        preds = self(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step for Lightning.\"\"\"\n",
    "        images, labels = batch\n",
    "\n",
    "        # Convert one-hot to class index if needed\n",
    "        labels = labels.argmax(dim=1) if labels.ndim == 2 else labels  \n",
    "\n",
    "        preds = self(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Test step for Lightning.\"\"\"\n",
    "        images, labels = batch\n",
    "\n",
    "        # Convert one-hot to class index if needed\n",
    "        labels = labels.argmax(dim=1) if labels.ndim == 2 else labels  \n",
    "\n",
    "        preds = self(images)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Optimizer & Scheduler setup.\"\"\"\n",
    "        optimizers = {\"adam\": optim.Adam, \"sgd\": optim.SGD}\n",
    "        optimizer = optimizers[self.hparams.optimizer_name](self.parameters(), lr=self.hparams.learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchsummary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, ModelCheckpoint\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorBoardLogger\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchsummary\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Instantiate DataModule\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data_module \u001b[38;5;241m=\u001b[39m CustomDataModule(data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_2/inaturalist_12K/train\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m                                use_augmentation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m                                val_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m,\n\u001b[1;32m     10\u001b[0m                                batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchsummary'"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchsummary import summary\n",
    "\n",
    "# Instantiate DataModule\n",
    "data_module = CustomDataModule(data_dir=\"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_2/inaturalist_12K/train\",\n",
    "                               use_augmentation=True,\n",
    "                               val_split=0.25,\n",
    "                               batch_size=64)\n",
    "\n",
    "# Call setup() to initialize datasets\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "# Instantiate Model with correct num_classes\n",
    "model = CustomCNN(num_classes=len(data_module.class_names),\n",
    "                  first_layer_filters=64,\n",
    "                  filter_org=1.0,\n",
    "                  kernel_size=3,\n",
    "                  conv_layers=5,\n",
    "                  activation=\"relu\",\n",
    "                  dropout=0.2,\n",
    "                  batch_norm=True,\n",
    "                  dense_size=128,\n",
    "                  learning_rate=1e-3,\n",
    "                  optimizer_name=\"adam\")\n",
    "\n",
    "# Print Detailed Model Summary with torchsummary\n",
    "print(\"Detailed Model Summary:\")\n",
    "summary(model, input_size=(3, 224, 224))  # Adjust the input size depending on your model\n",
    "\n",
    "# Callbacks for Training\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "checkpoint = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=1, monitor=\"val_loss\", mode=\"min\")\n",
    "\n",
    "# Setup TensorBoardLogger for better logging and visualization\n",
    "logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\")\n",
    "\n",
    "# Train the Model\n",
    "trainer = Trainer(\n",
    "    max_epochs=5,\n",
    "    precision=16,  # Mixed precision for speed\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "# Test Model\n",
    "trainer.test(model, datamodule=data_module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
