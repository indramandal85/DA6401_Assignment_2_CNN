{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d40z17eUXqIV",
        "outputId": "285d8dd3-5893-46a2-aaf4-a34af47d5150"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.5.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.7.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch_lightning) (4.13.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.14.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.11.14)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.1.0->pytorch_lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Downloading pytorch_lightning-2.5.1-py3-none-any.whl (822 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.2-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.0-py3-none-any.whl (960 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m960.9/960.9 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch_lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch_lightning-2.5.1 torchmetrics-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7XqypgADW58H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import wandb\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from torchsummary import summary\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.tuner.tuning import Tuner\n",
        "import random\n",
        "wandb.login(key = '5df7feeffbc5b918c8947f5fe4bab4b67ebfbb69')\n",
        "# key = 5df7feeffbc5b918c8947f5fe4bab4b67ebfbb69"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ogaB7lIaW58J"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CustomDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_dir: str,\n",
        "        image_size: Tuple[int, int] = (224, 224),\n",
        "        batch_size: int = 64,\n",
        "        val_split: float = 0.2,\n",
        "        use_augmentation: bool = False,\n",
        "        num_workers: int = 4,\n",
        "        seed: int = 42\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Custom Data Module for handling dataset loading, transformation, and splitting.\n",
        "\n",
        "        Args:\n",
        "            data_dir (str): Path to dataset directory.\n",
        "            image_size (Tuple[int, int]): Target image size (height, width).\n",
        "            batch_size (int): Batch size for DataLoader.\n",
        "            val_split (float): Fraction of training data to use for validation.\n",
        "            use_augmentation (bool): Whether to apply data augmentation.\n",
        "            num_workers (int): Number of workers for DataLoader.\n",
        "            seed (int): Random seed for reproducibility.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.image_size = image_size\n",
        "        self.batch_size = batch_size\n",
        "        self.val_split = val_split\n",
        "        self.use_augmentation = use_augmentation\n",
        "        self.num_workers = num_workers\n",
        "        self.seed = seed\n",
        "        self.class_names = []\n",
        "\n",
        "        # Define transforms\n",
        "        self.train_transform = self._get_train_transform()\n",
        "        self.test_transform = self._get_test_transform()\n",
        "\n",
        "        # Set manual seeds for reproducibility\n",
        "        torch.manual_seed(self.seed)\n",
        "        np.random.seed(self.seed)\n",
        "\n",
        "    def _get_train_transform(self):\n",
        "        \"\"\"Defines transformation pipeline for training data.\"\"\"\n",
        "        if self.use_augmentation:\n",
        "            return transforms.Compose([\n",
        "                transforms.RandomResizedCrop(self.image_size[0]),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomVerticalFlip(),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), shear=10),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize(self.image_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "    def _get_test_transform(self):\n",
        "        \"\"\"Defines transformation pipeline for validation and test data.\"\"\"\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(self.image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def _split_train_val(self, dataset):\n",
        "        \"\"\"Splits dataset into training and validation sets while preserving class distribution.\"\"\"\n",
        "        total_size = len(dataset)\n",
        "        indices = torch.randperm(total_size).tolist()\n",
        "\n",
        "        val_size = int(total_size * self.val_split)\n",
        "        train_indices, val_indices = indices[val_size:], indices[:val_size]\n",
        "\n",
        "        return train_indices, val_indices\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None):\n",
        "        \"\"\"Loads datasets and applies transformations.\"\"\"\n",
        "        full_dataset = datasets.ImageFolder(root=self.data_dir, transform=self.train_transform)\n",
        "        self.class_names = full_dataset.classes\n",
        "\n",
        "        train_idx, val_idx = self._split_train_val(full_dataset)\n",
        "\n",
        "        # Create subsets\n",
        "        self.train_dataset = Subset(full_dataset, train_idx)\n",
        "        self.val_dataset = Subset(datasets.ImageFolder(root=self.data_dir, transform=self.test_transform), val_idx)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Returns DataLoader for training data.\"\"\"\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"Returns DataLoader for validation data.\"\"\"\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self, test_dir: Optional[str] = None):\n",
        "        \"\"\"Returns DataLoader for test data.\"\"\"\n",
        "        test_path = Path(test_dir) if test_dir else self.data_dir.parent / \"val\"\n",
        "        test_dataset = datasets.ImageFolder(root=test_path, transform=self.test_transform)\n",
        "        return DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4jF-9DzW58K"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "'''\n",
        "\n",
        "data_module = CustomDataModule(data_dir=\"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_2/inaturalist_12K/train\",\n",
        "                                use_augmentation=True,\n",
        "                                val_split= 0.2,\n",
        "                                batch_size= 64,\n",
        "                                seed= 42,\n",
        "                                )\n",
        "data_module.setup()\n",
        "\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()\n",
        "test_loader = data_module.test_dataloader()\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "print(f\"Test batches: {len(test_loader)}\")\n",
        "print(f\"Class Names: {data_module.class_names}\")'\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wXvGewSiW58K"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class CustomCNN(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape: Tuple[int, int, int] = (3, 224, 224),\n",
        "        num_classes: int = 10,\n",
        "        first_layer_filters: int = 32,\n",
        "        filter_org: float = 2.0,\n",
        "        kernel_size: int = 3,\n",
        "        conv_layers: int = 4,\n",
        "        activation: str = \"relu\",\n",
        "        dropout: float = 0.3,\n",
        "        batch_norm: bool = True,\n",
        "        dense_size: int = 128,\n",
        "        learning_rate: float = 1e-3,\n",
        "        optimizer_name: str = \"adam\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Custom CNN Model\n",
        "\n",
        "        Args:\n",
        "            input_shape (Tuple[int, int, int]): (Channels, Height, Width)\n",
        "            num_classes (int): Number of output classes\n",
        "            first_layer_filters (int): Number of filters in the first layer\n",
        "            filter_org (float): Scaling factor for filters per layer\n",
        "            kernel_size (int): Size of the convolution kernel\n",
        "            conv_layers (int): Number of convolutional layers\n",
        "            activation (str): Activation function to use (relu, gelu, mish, silu)\n",
        "            dropout (float): Dropout rate\n",
        "            batch_norm (bool): Whether to use batch normalization\n",
        "            dense_size (int): Number of neurons in the fully connected layer\n",
        "            learning_rate (float): Learning rate\n",
        "            optimizer_name (str): Optimizer type (adam, sgd, etc.)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Choose activation function\n",
        "        activations = {\n",
        "            \"relu\": nn.ReLU(),\n",
        "            \"gelu\": nn.GELU(),\n",
        "            \"silu\": nn.SiLU(),\n",
        "            \"mish\": nn.Mish()\n",
        "        }\n",
        "        self.activation = activations.get(activation, nn.ReLU())\n",
        "\n",
        "        # CNN Layers\n",
        "        layers = []\n",
        "        in_channels = input_shape[0]\n",
        "        filters = first_layer_filters\n",
        "\n",
        "        for _ in range(conv_layers):\n",
        "            layers.append(nn.Conv2d(in_channels, filters, kernel_size, padding=1))\n",
        "            if batch_norm:\n",
        "                layers.append(nn.BatchNorm2d(filters))\n",
        "            layers.append(self.activation)\n",
        "            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            in_channels = filters\n",
        "            filters = int(filters * filter_org)  # Ensure filters remain an integer\n",
        "\n",
        "        self.conv_block = nn.Sequential(*layers)\n",
        "\n",
        "        # Calculate output feature map size\n",
        "        feature_map_size = input_shape[1] // (2 ** conv_layers)  # Assuming max pooling halves the size each time\n",
        "        if feature_map_size <= 0:\n",
        "            raise ValueError(\"Too many pooling layers, feature map size is zero!\")\n",
        "\n",
        "        final_filters = in_channels  # Last number of filters used in conv layers\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(in_features=int(final_filters * feature_map_size * feature_map_size), out_features=dense_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(dense_size, num_classes)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Defines the forward pass.\"\"\"\n",
        "        x = self.conv_block(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"Training step for Lightning.\"\"\"\n",
        "        images, labels = batch\n",
        "\n",
        "        # Convert one-hot to class index if needed\n",
        "        labels = labels.argmax(dim=1) if labels.ndim == 2 else labels\n",
        "\n",
        "        preds = self(images)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
        "        self.log(\"train_loss\", loss, prog_bar=True)\n",
        "        self.log(\"train_acc\", acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"Validation step for Lightning.\"\"\"\n",
        "        images, labels = batch\n",
        "\n",
        "        # Convert one-hot to class index if needed\n",
        "        labels = labels.argmax(dim=1) if labels.ndim == 2 else labels\n",
        "\n",
        "        preds = self(images)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
        "        self.log(\"val_loss\", loss, prog_bar=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"Test step for Lightning.\"\"\"\n",
        "        images, labels = batch\n",
        "\n",
        "        # Convert one-hot to class index if needed\n",
        "        labels = labels.argmax(dim=1) if labels.ndim == 2 else labels\n",
        "\n",
        "        preds = self(images)\n",
        "        loss = F.cross_entropy(preds, labels)\n",
        "        acc = (preds.argmax(dim=1) == labels).float().mean()\n",
        "        self.log(\"test_loss\", loss, prog_bar=True)\n",
        "        self.log(\"test_acc\", acc, prog_bar=True)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"Optimizer & Scheduler setup.\"\"\"\n",
        "        optimizers = {\"adam\": optim.Adam, \"sgd\": optim.SGD}\n",
        "        optimizer = optimizers[self.hparams.optimizer_name](self.parameters(), lr=self.hparams.learning_rate)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "        return [optimizer], [scheduler]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8FA7hjGYmtu",
        "outputId": "61afee7f-5f60-4067-fcc5-4cbafc98294e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfrX7CV4W58L"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# Instantiate DataModule\n",
        "data_module = CustomDataModule(data_dir=\"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_2/inaturalist_12K/train\",\n",
        "                               use_augmentation=True,\n",
        "                               val_split=0.25,\n",
        "                               batch_size=64,\n",
        "                               seed=42,\n",
        "                               )\n",
        "\n",
        "# Call setup() to initialize datasets\n",
        "data_module.setup(stage=\"fit\")\n",
        "\n",
        "# Instantiate Model with correct num_classes\n",
        "model = CustomCNN(num_classes=len(data_module.class_names),\n",
        "                  first_layer_filters=64,\n",
        "                  filter_org=1.0,\n",
        "                  kernel_size=3,\n",
        "                  conv_layers=5,\n",
        "                  activation=\"relu\",\n",
        "                  dropout=0.2,\n",
        "                  batch_norm=True,\n",
        "                  dense_size=128,\n",
        "                  learning_rate=1e-3,\n",
        "                  optimizer_name=\"adam\")\n",
        "\n",
        "# Print Detailed Model Summary with torchsummary\n",
        "print(\"Detailed Model Summary:\")\n",
        "summary(model, input_size=(3, 224, 224))  # Adjust the input size depending on your model\n",
        "\n",
        "# Callbacks for Training\n",
        "early_stop = EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\")\n",
        "# checkpoint = ModelCheckpoint(dirpath=\"checkpoints/\", save_top_k=1, monitor=\"val_loss\", mode=\"min\")\n",
        "\n",
        "\n",
        "# Train the Model\n",
        "trainer = Trainer(\n",
        "    max_epochs=20,\n",
        "    precision=16,  # Mixed precision for speed\n",
        "    callbacks=[early_stop], # [checkpoint],\n",
        "    enable_progress_bar=True,\n",
        ")\n",
        "\n",
        "trainer.fit(model, datamodule=data_module)\n",
        "\n",
        "# Test Model\n",
        "# trainer.test(model, datamodule=data_module)\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0saKq7jW58L"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define sweep configuration\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",  # Bayesian optimization for efficiency\n",
        "    \"metric\": {\"name\": \"val_acc\", \"goal\": \"maximize\"},  # Optimize validation accuracy\n",
        "    \"parameters\": {\n",
        "        \"first_layer_filters\": {\"values\": [32, 64, 128]},  # Number of filters in the first layer\n",
        "        \"filter_org\": {\"values\": [1.0, 2.0, 0.5]},  # Filter organization strategy\n",
        "        \"conv_layers\": {\"values\": [3, 4, 5]},  # Number of convolutional layers\n",
        "        \"activation\": {\"values\": [\"relu\", \"gelu\", \"silu\", \"mish\"]},  # Activation functions\n",
        "        \"dropout\": {\"values\": [0.2, 0.3]},  # Dropout rate\n",
        "        \"batch_norm\": {\"values\": [True, False]},  # Batch normalization\n",
        "        \"batch_size\": {\"values\": [32, 64]},  # Batch size\n",
        "        \"learning_rate\": {\"values\": [1e-2, 1e-3, 1e-4]},  # Learning rates\n",
        "        \"use_augmentation\": {\"values\": [True, False]},  # Data augmentation\n",
        "        \"num_neurons_dense\": {\"values\": [128, 256, 512]},  # Dense layer neurons\n",
        "    },\n",
        "}\n",
        "\n",
        "# Initialize sweep with entity\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"CNN_Hyperparameter_Tuning_3\")\n",
        "\n",
        "# Define training function for each sweep run\n",
        "def train():\n",
        "    wandb.init()  # Initialize a new wandb run\n",
        "\n",
        "    # Set run name dynamically\n",
        "    run_name = (f\"-ac-{wandb.config.activation}\"\n",
        "                f\"-filters-{wandb.config.first_layer_filters}\"\n",
        "                f\"-filt_org-{wandb.config.filter_org}\"\n",
        "                f\"-conv_layers-{wandb.config.conv_layers}\"\n",
        "                f\"-dropout-{wandb.config.dropout}\"\n",
        "                f\"-batch_norm-{wandb.config.batch_norm}\"\n",
        "                f\"-data_aug-{wandb.config.use_augmentation}\"\n",
        "                f\"-num_neurons_dense-{wandb.config.num_neurons_dense}\")\n",
        "\n",
        "    wandb.run.name = run_name  # Assign custom run name\n",
        "\n",
        "    # Instantiate DataModule with selected parameters\n",
        "    data_module = CustomDataModule(\n",
        "        data_dir=\"/Users/indramandal/Documents/VS_CODE/DA6401/DA6401_Assignment_2/inaturalist_12K/train\",\n",
        "        use_augmentation=wandb.config.use_augmentation,\n",
        "        val_split=0.2,  # Use 20% of training data for validation\n",
        "        batch_size=wandb.config.batch_size\n",
        "    )\n",
        "    data_module.setup(stage=\"fit\")\n",
        "\n",
        "    # Instantiate CNN Model with hyperparameters from wandb\n",
        "    model = CustomCNN(\n",
        "        num_classes=len(data_module.class_names),\n",
        "        first_layer_filters=wandb.config.first_layer_filters,\n",
        "        filter_org=wandb.config.filter_org,\n",
        "        kernel_size=3,\n",
        "        conv_layers=wandb.config.conv_layers,\n",
        "        activation=wandb.config.activation,\n",
        "        dropout=wandb.config.dropout,\n",
        "        batch_norm=wandb.config.batch_norm,\n",
        "        dense_size=wandb.config.num_neurons_dense,  # Dense layer neurons\n",
        "        learning_rate=wandb.config.learning_rate,\n",
        "        optimizer_name=\"adam\"\n",
        "    )\n",
        "\n",
        "    # Set up WandbLogger with entity\n",
        "    wandb_logger = WandbLogger(project=\"CNN_Hyperparameter_Tuning_3\", entity=\"Indra_Mandal_ED24S014\")\n",
        "\n",
        "    # Callbacks\n",
        "    early_stop = EarlyStopping(monitor=\"val_acc\", patience=5, mode=\"max\")\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        max_epochs=50,\n",
        "        precision=16,  # Mixed precision for speed\n",
        "        callbacks=[early_stop],\n",
        "        enable_progress_bar=True,\n",
        "        logger=wandb_logger\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    trainer.fit(model, datamodule=data_module)\n",
        "\n",
        "    wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ezveQL_W58L"
      },
      "outputs": [],
      "source": [
        "# key = 5df7feeffbc5b918c8947f5fe4bab4b67ebfbb69\n",
        "# Launch Sweep Agent\n",
        "wandb.agent(sweep_id, train , count=50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}